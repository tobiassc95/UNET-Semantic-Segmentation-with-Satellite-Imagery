{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # To walk through the directories.\n",
    "import numpy as np # To manipulate arrays.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2 # OpenCV to read images.\n",
    "from patchify import patchify # To divide the images into smaller patches.\n",
    "from PIL import Image # To perform cropping or resizing operations in an image.\n",
    "\n",
    "images_path = \"Dataset/MassachusettsBuildings/Images\"\n",
    "\n",
    "images_list = []\n",
    "patch_size = 256 # To divide the images into 256 patch size.\n",
    "image_files = os.listdir(images_path) # Gets a list of all the files in the path.\n",
    "for image_file in image_files:\n",
    "    image = cv2.imread(images_path + '/' + image_file, cv2.IMREAD_COLOR)  # Reads each image as BGR.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Converts each image as RGB.\n",
    "    image_dim = (0, 0, (image.shape[1]//patch_size)*patch_size, (image.shape[0]//patch_size)*patch_size) # Nearest size divisible by 256 (patch size).\n",
    "    image = Image.fromarray(image) # Converts the image into a PIL image.\n",
    "    image = image.crop(image_dim)  # Crops the image from the top left corner.\n",
    "    image = np.array(image) # Converts the image into a numpy array.\n",
    "\n",
    "    image_patches = patchify(image, (patch_size, patch_size, 3), step=patch_size)  # There is no overlap since the step=PatchSize.\n",
    "    for i in range(image_patches.shape[0]):\n",
    "        for j in range(image_patches.shape[1]):\n",
    "            image_patch = image_patches[i,j] # Gets each patched image.\n",
    "            \n",
    "            # #Use minmaxscaler instead of just dividing by 255. \n",
    "            # single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape\n",
    "            # # single_patch_img = (single_patch_img.astype('float32')) / 255.\n",
    "\n",
    "            image_patch = image_patch[0] # Drop the extra unecessary dimension that patchify adds.\n",
    "            images_list.append(image_patch) # Adds each patched image into the dataset list.\n",
    "\n",
    "images_list = np.array(images_list) # Converts the dataset into a numpy array.\n",
    "print(f\"Dimension of image data: {images_list.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "targets_path = \"Dataset/MassachusettsBuildings/Targets\"\n",
    "\n",
    "targets_list = []\n",
    "patch_size = 256 # To divide the images into 256 patch size.\n",
    "target_files = os.listdir(targets_path) # Gets a list of all the files in the path.\n",
    "for target_file in target_files:\n",
    "    target = cv2.imread(targets_path + '/' + target_file, cv2.IMREAD_COLOR)  # Read each image as BGR.\n",
    "    target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB) # Converts each image as RGB.\n",
    "    target_dim = (0, 0, (target.shape[1]//patch_size)*patch_size, (target.shape[0]//patch_size)*patch_size) # Nearest size divisible by 256 (patch size).\n",
    "    target = Image.fromarray(target) # Converts the image into a PIL image.\n",
    "    target = target.crop(target_dim)  # Crops the image from the top left corner.\n",
    "    target = np.array(target) # Converts the image into a numpy array.\n",
    "\n",
    "    target_patches = patchify(target, (patch_size, patch_size, 3), step=patch_size)  # Step = 256 for 256 patches means no overlap\n",
    "    for i in range(target_patches.shape[0]):\n",
    "        for j in range(target_patches.shape[1]):\n",
    "            target_patch = target_patches[i,j] # targetPatches[i][j][:][:] # Gets each patched image.\n",
    "            \n",
    "            # #Use minmaxscaler instead of just dividing by 255. \n",
    "            # single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape\n",
    "            # targetPatch = targetPatch.astype(\"float32\")/255.\n",
    "\n",
    "            target_patch = target_patch[0] # Drop the extra unecessary dimension that patchify adds.                               \n",
    "            targets_list.append(target_patch) # Adds each patched image into the dataset list.\n",
    "\n",
    "targets_list = np.array(targets_list) # Converts the dataset into a numpy array.\n",
    "print(f\"Dimension of target data: {targets_list.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "targets_list = (targets_list[:,:,:,0]/255).astype(\"int8\") # Converts into one hot encoded vector (just the first channel, no need for all 3 channels). \n",
    "targets_list = np.array(targets_list) # Converts the dataset into a numpy array.\n",
    "targets_list = np.expand_dims(targets_list, axis=3) # Expands the dimension at axis 3 to match with the dataset.\n",
    "print(f\"Dimension of target labels: {targets_list.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainset after spliting the dateset: 3375.\n",
      "Size of testset after spliting the dateset: 375.\n",
      "Size of train loader (with batch size 32): 106.\n",
      "Size of test loader (with batch size 32): 12.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images_list, targets_list, test_size=0.1, random_state=375)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.int8))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.int8))\n",
    "\n",
    "trainset = TensorDataset(x_train, y_train) # Creates your dataset.\n",
    "testset = TensorDataset(x_test, y_test) # Creates your dataset.\n",
    "\n",
    "print(f\"Size of trainset after spliting the dateset: {len(trainset)}.\")\n",
    "print(f\"Size of testset after spliting the dateset: {len(testset)}.\")\n",
    "\n",
    "batch_size = 32 # Hyperparameter.\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4) # shuffle=True to reshuffle at every epoch.\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=4) # shuffle=True to reshuffle at every epoch.\n",
    "\n",
    "print(f\"Size of train loader (with batch size {batch_size}): {len(train_loader)}.\") # ceil(len(trainset)/batch_size).\n",
    "print(f\"Size of test loader (with batch size {batch_size}): {len(test_loader)}.\") # ceil(len(testset)/batch_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from UNET_Model.unet import Unet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (f\"Device being used: {device}\")\n",
    "\n",
    "epochs = 10 # Hyperparameter.\n",
    "lr = 0.001 # Hyperparameter.\n",
    "\n",
    "model = Unet(in_channels=3, n_classes=1, is_batchnorm=True)\n",
    "model = model.to(device=device) # Move the model parameters from CPU to GPU.\n",
    "\n",
    "criterion = nn.BCELoss() # Loss function.\n",
    "criterion = criterion.to(device=device) # Move the model parameters from CPU to GPU.\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) # Hyperparameter.\n",
    "# optimizer = optim.SGD(Model.parameters(), lr=LearningRate) # Hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(Device)\n",
    "        targets = targets.to(Device)\n",
    "\n",
    "        # Forward pass.\n",
    "        predicts = model(targets)\n",
    "        loss = criterion(predicts, targets)\n",
    "\n",
    "        # Backward and optimize.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        _, predicts = torch.max(predicts, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Epoch: {epoch+1}/{epochs}, Step: {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finished Training')\n",
    "PATH = './unet.pth'\n",
    "torch.save(Model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(Device)\n",
    "        labels = labels.to(Device)\n",
    "        outputs = Model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(BatchSize):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 100.0 * n_correct / n_samples\n",
    "print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "for i in range(10):\n",
    "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "    print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
